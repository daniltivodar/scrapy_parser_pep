# Парсер документов PEP на Scrapy

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Scrapy](https://img.shields.io/badge/Scrapy-2.5+-green.svg)](https://scrapy.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Производительный парсер для сбора и структурирования информации из документов PEP (Python Enhancement Proposal). Проект предназначен для автоматизации процесса сбора актуальных данных о всех спецификациях Python.

## Возможности

### Основной функционал
- Сбор полной информации о документах PEP
- Обход официальных источников python.org
- Структурирование и сохранение данных

### Обработка данных
- Извлечение номеров, названий и статусов PEP
- Сбор информации об авторах и содержании
- Валидация и очистка полученных данных

### Экспорт результатов
- Сохранение данных в структурированном CSV формате
- Подготовка данных для импорта в базы данных
- Генерация отчетов для последующего анализа

## Технологический стек

- Python 3.9+ - Основной язык программирования
- Scrapy 2.5+ - Фреймворк для парсинга веб-сайтов
- CSV - Формат для хранения структурированных данных

## Быстрый старт

### Предварительные требования
- Python 3.9 или новее
- Виртуальное окружение

### Установка и настройка

1. Клонируйте репозиторий:
```bash
git clone https://github.com/daniltivodar/bs4_parser_pep.git
cd bs4_parser_pep
```

2. Создайте и активируйте виртуальное окружение:
```bash
python -m venv venv
source venv/bin/activate  # Linux/MacOS
venv\Scripts\activate  # Windows
```

3. Установите зависимости:
```bash
pip install -r requirements.txt
```

4. Запустите парсер:
```bash
scrapy crawl pep
```

## Использование

После запуска парсера данные будут сохранены в CSV файле с указанием:
- Номера и названия каждого PEP
- Текущего статуса документа
- Авторов и участников
- Даты создания и обновления
- Ссылок на оригинальные документы

## Разработчик

**Данил Тиводар**  
[GitHub Профиль](https://github.com/daniltivodar)
